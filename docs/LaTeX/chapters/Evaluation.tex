\section{Evaluation}
In this section, we present the evaluation results, for both $\textit{prefix\_length}=5$ and $\textit{prefix\_length}=10$, of the decision tree classifier and the recommendation system built on top of it.

\subsection{Hyperparameters}
Tables~\ref{tab:hyperparameters_5} and~\ref{tab:hyperparameters_10} show the optimized hyperparameters obtained through \textit{Hyperopt} for both prefix lengths. 
\\\\
For $\textit{prefix\_length}=5$, the optimizer selects a shallow tree with a maximum depth of $3$ and a small number of features considered at each split.
This choice limits overfitting in a setting where only partial information about the trace is available, while still allowing the model to exploit the most informative activities.
The use of the entropy criterion leads to more balanced and interpretable decision paths.
\\\\
For $\textit{prefix\_length}=10$, the optimal configuration still constrains the maximum depth to $3$, confirming that deeper trees do not provide additional benefits even when more information is available.
However, the number of maximum features increases, indicating that longer prefixes allow the model to effectively leverage a richer set of activities.
In this case, the Gini criterion is selected, suggesting that it better captures class separation when more contextual information is present.

\begin{table}[ht]
    \centering
    \begin{minipage}[t]{0.45\textwidth}
        \centering
        \begin{tabular}{l c}
            \hline
            \textbf{Hyperparameter} & \textbf{Value} \\
            \hline
            Criterion & Entropy \\
            Max Depth & 3 \\
            Max Features & 2 \\
            \hline
        \end{tabular}
        \caption{Optimized hyperparameters for the decision tree classifier for $\textit{prefix\_length}=5$.}
        \label{tab:hyperparameters_5}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.45\textwidth}
        \centering
        \begin{tabular}{l c}
            \hline
            \textbf{Hyperparameter} & \textbf{Value} \\
            \hline
            Criterion & Gini \\
            Max Depth & 3 \\
            Max Features & 3 \\
            \hline
        \end{tabular}
        \caption{Optimized hyperparameters for the decision tree classifier for $\textit{prefix\_length}=10$.}
        \label{tab:hyperparameters_10}
    \end{minipage}
\end{table}


\subsection{Decision Tree Structure}
Figures~\ref{fig:decision_tree_5} and~\ref{fig:decision_tree_10} illustrate the structure of the trained decision trees.
The color-coded leaf nodes (blue for positive outcomes, orange for negative outcomes) clearly show the distribution of predictions across different decision paths. 
Both trees maintain interpretability thanks to their depth, as the controlled \textit{max\_depth} and \textit{max\_features} parameters prevent excessive branching.
The tree structures provide valuable insights into which activities are most discriminative for predicting case outcomes, making the model transparent.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{../media/decision_tree_5.png}
    \caption{Visualization of the trained decision tree classifier for $\textit{prefix\_length}=5$. Each node represents a decision based on the presence or absence of specific activities, leading to recommendations at the leaf nodes.}
    \label{fig:decision_tree_5}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{../media/decision_tree_10.png}
    \caption{Visualization of the trained decision tree classifier for $\textit{prefix\_length}=10$. }
    \label{fig:decision_tree_10}
\end{figure}

\subsubsection{Evaluation Metrics}

The classification results in Tables~\ref{tab:evaluation_metrics_5} and~\ref{tab:evaluation_metrics_10} demonstrate the predictive performance of our decision tree models. 
\\\\
For $\textit{prefix\_length}=5$, these results indicate a reasonably balanced performance when only a limited portion of each trace is available.
In this setting, the model is already able to capture meaningful patterns, but the reduced amount of information limits its power.
\\\\
The performance further improves for $\textit{prefix\_length}=10$, where the results suggest that longer prefixes provide more informative context, enabling the model to make more confident positive predictions.
Overall, the results confirm that increasing the \textit{prefix\_length} has a positive impact on outcome prediction, as more activities contribute to defining the case trajectory.
\\\\
The confusion matrices in Figures~\ref{fig:confusion_matrix_5} and~\ref{fig:confusion_matrix_10} provide further insight into the distribution of predictions and misclassifications.

\begin{figure}[H]
    \centering
    \begin{minipage}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../media/confusion_matrix_5.png}
        \caption{Confusion matrix of predictions in the test set ($\textit{prefix\_length}=5$).}
        \label{fig:confusion_matrix_5}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../media/confusion_matrix_10.png}
        \caption{Confusion matrix of predictions in the test set ($\textit{prefix\_length}=10$).}
        \label{fig:confusion_matrix_10}
    \end{minipage}
\end{figure}

\begin{table}[H]
    \centering
    \begin{minipage}[t]{0.45\textwidth}
        \centering
        \begin{tabular}{l c}
            \hline
            \textbf{Metric} & \textbf{Value} \\
            \hline
            Accuracy & 69.77\% \\
            Precision & 64.84\% \\
            Recall & 69.77\% \\
            F1-Score & 66.42\% \\
            \hline
        \end{tabular}
        \caption{Evaluation metrics for the decision tree classifier on the test set for $\textit{prefix\_length}=5$.}
        \label{tab:evaluation_metrics_5}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.45\textwidth}
        \centering
        \begin{tabular}{l c}
            \hline
            \textbf{Metric} & \textbf{Value} \\
            \hline
            Accuracy & 74.42\% \\
            Precision & 83.97\% \\
            Recall & 74.42\% \\
            F1-Score & 76.04\% \\
            \hline
        \end{tabular}
        \caption{Evaluation metrics for the decision tree classifier on the test set for $\textit{prefix\_length}=10$.}
        \label{tab:evaluation_metrics_10}
    \end{minipage}
\end{table}

\subsection{Recommendation Analysis}

\paragraph{Recommendation Generation}
For the sake of simplicity in explaining the recommendation extraction process, we chose to consider the tree in Figure~\ref{fig:decision_tree_recommendation}.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{../media/decision_tree_small.png}
    \caption{Simplified decision tree used to illustrate the recommendation extraction process.
    Given a node, the left child branch is taken if the condition is true (activity is absent in the trace), while the right child branch is taken if the condition is false (activity is present in the trace).}
    \label{fig:decision_tree_recommendation}
\end{figure}
Applying the extraction of positive paths (Algorithm~\ref{alg:get_positive_paths}) to this tree, we obtain the following positive paths with their confidence scores:
\begin{itemize}
    \item Path 1: \textit{Turning Q.C.} = F $\wedge$ \textit{Grinding Rework - Machine 27} = F $\wedge$ \textit{Turning - Machine 9} = F $\wedge$ \textit{Turning \& Milling Q.C.} = F with confidence $0.77$;
    \item Path 2: \textit{Turning Q.C.} = F $\wedge$ \textit{Grinding Rework - Machine 27} = F $\wedge$ \textit{Turning - Machine 9} = T with confidence $0.80$;
    \item Path 3: \textit{Turning Q.C.} = T $\wedge$ \textit{Turning \& Milling - Machine 10} = F $\wedge$ \textit{Final Inspection Q.C.} = F $\wedge$ \textit{Packing} = T with confidence $1.0$;
    \item Path 4: \textit{Turning Q.C.} = T $\wedge$ \textit{Turning \& Milling - Machine 10} = T with confidence $1.0$;
\end{itemize}
Now, suppose a prefix trace that has the following activities: \{\textit{Final Inspection Q.C.}: True, \textit{Packing}: True, \textit{Grinding Rework - Machine 27}: True\}, represented by the orange path in Figure~\ref{fig:decision_tree_recommendation} is predicted as false.
To find the recommendations for this trace, we first filter the positive paths to find the compliant ones.
\begin{itemize}
    \item Path 1 and Path 2 are not compliant because they require \textit{Grinding Rework - Machine 27} = F, which is violated by the trace;
    \item Path 3 is compliant because none of its conditions are violated by the trace;
    \item Path 4 is also not compliant because it requires \textit{Final Inspection Q.C.} = F, which is violated by the trace.
\end{itemize}
Given this, we can derive the recommendations for the given prefix trace as the activities that need to be added to follow Path 3.
Since both \textit{Turning Q.C.} and \textit{Turning \& Milling - Machine 10} need to be true to follow Path 3, the recommended activities will be them.
This is illustrated in Figure~\ref{fig:recommendation_tree}, with the green dotted nodes.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{../media/recommendation_tree.png}
    \caption{The blue squared nodes represent the activities present in the given prefix trace. The green highlighted node represent the recommended activities}
    \label{fig:recommendation_tree}
\end{figure}

\paragraph{Recommendations Quality}
Tables~\ref{tab:evaluation_metrics_rec_5} and~\ref{tab:evaluation_metrics_rec_10} present the evaluation of our recommendation system.
This values have to be considered as an approximation, since the ground truth is based on actual traces rather than hypothetical scenarios where recommendations were followed.
\\\\
For $\textit{prefix\_length}=5$, the recommendation system achieves moderate accuracy with balanced precision and recall.
This indicates that, even with limited information, the system provides reasonably reliable recommendations that align with positive outcomes.
The balanced precision and recall suggest that the system strikes a reasonable trade-off between suggesting necessary actions and avoiding over-recommendation, demonstrating practical utility in this early-stage setting.
\\\\
At $\textit{prefix\_length}=10$, both accuracy and precision improve notably, while recall remains relatively stable.
This behavior indicates that with more contextual information available, the system becomes more confident and accurate in its recommendations.
The increased precision shows that the recommendations are more targeted and relevant, while the maintained recall ensures that important suggestions are still captured.
These results confirm that the recommendation system becomes more effective and reliable as more trace information becomes available, supporting its practical usage in real-world scenarios.
\begin{table}[H]
    \centering
    \begin{minipage}[t]{0.45\textwidth}
        \centering
        \begin{tabular}{l c}
            \hline
            \textbf{Metric} & \textbf{Value} \\
            \hline
            Accuracy & 63.64\% \\
            Precision & 30.43\% \\
            Recall & 41.18\% \\
            F1-Score & 28.57\% \\
            \hline
        \end{tabular}
        \caption{Evaluation metrics for the recommendation system on the test set for $\textit{prefix\_length}=5$.}
        \label{tab:evaluation_metrics_rec_5}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.45\textwidth}
        \centering
        \begin{tabular}{l c}
            \hline
            \textbf{Metric} & \textbf{Value} \\
            \hline
            Accuracy & 58.33\% \\
            Precision & 41.18\% \\\
            Recall & 48.28\% \\
            F1-Score & 51.61\% \\
            \hline
        \end{tabular}
        \caption{Evaluation metrics for the recommendation system on the test set for $\textit{prefix\_length}=10$.}
        \label{tab:evaluation_metrics_rec_10}
    \end{minipage}
\end{table}

For a more detailed understanding of the system's performance, we present the confusion matrices in Figures~\ref{fig:rec_confusion_matrix_5} and~\ref{fig:rec_confusion_matrix_10}.
\begin{figure}[H]
    \centering
    \begin{minipage}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../media/recommendation_confusion_matrix_5.png}
        \caption{Confusion matrix of recommendations($\textit{prefix\_length}=5$).}
        \label{fig:rec_confusion_matrix_5}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../media/recommendation_confusion_matrix_10.png}
        \caption{Confusion matrix of recommendations($\textit{prefix\_length}=10$).}
        \label{fig:rec_confusion_matrix_10}
    \end{minipage}
\end{figure}