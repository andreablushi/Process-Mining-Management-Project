\section{Implementation}
A recommendation system was implemented using a decision tree classifier to analyze production event logs.
The implementation was achieved in Python, leveraging libraries such as \textit{PM4Py} for process mining tasks and \textit{scikit-learn} for machine learning functionalities.
The execution flow of our implementation resides within the \textit{notebook.ipynb} file, which calls all the designed utility functions.

\subsection{Encoding the Event Log}
We began by importing the event log file, which is in \textit{XES} format, using the \textit{PM4Py} library. 
We then converted this file into a \textit{PM4Py} event log object suitable for subsequent process mining tasks.
\\\\
A pruned copy of the event log was created to consider only the prefixes of each trace.
To achieve this, each trace in the log was truncated to \textit{prefix\_length} events.
Since the partial ongoing trace in a real-world scenario is incomplete, we trained the classifier 
exclusively on the prefixes of the traces.
\\\\
Next, we extracted the unique activity names from the complete event log to serve as columns (features) for the encoding step.
In addition to these, we included the trace identifier, the prefix length, and the ground truth label, 
which indicates whether the trace outcome was positive or negative.
\\\\
To prepare this data for training a decision tree classifier, the pruned events were encoded using a \textit{Boolean encoding} scheme.
This encoding involved creating binary features for each unique activity name. 
For each trace, if an activity was present within the trace, the corresponding feature in that row was set to true (1); otherwise, it was set to false (0).
This approach allows for the creation of a feature set with a fixed number of columns, eliminating the need to pad traces to a uniform length.
Upon completion, we obtained a \textit{DataFrame} in which each row represented a trace, while the feature columns indicated the presence or absence of specific activities within that trace.

\subsection{Training the Decision Tree Classifier}
Following the encoding of the pruned event log, a decision tree classifier was trained using the \textit{scikit-learn} library.  
We separated the encoded log into features $(\boldsymbol{X})$, which were derived by removing the trace identifier and label columns, and labels $(\boldsymbol{y})$, which represented the ground truth extracted directly from the log.  
The dataset had been pre-partitioned in the original \textit{XES} file into distinct training and testing sets; these partitions were used to train and evaluate the classifier, respectively.
\\\\
After training the classifier, it was possible to visualize the learned decision rules by plotting the decision tree using built-in \textit{scikit-learn} functionality.  

\subsection{Optimizing the Classifier Hyperparameters}
To enhance the performance of our decision tree classifier, we employed the \textit{Hyperopt} library to perform hyperparameter optimization.
This tool systematically explores a defined search space, iteratively testing combinations of critical hyperparameters—such as \textit{max\_depth}, \textit{max\_features}, and the split criterion (e.g., \textit{Gini} or \textit{entropy})—to identify the optimal configuration.
At every iteration, the model was trained and evaluated using the $F_1$-score as the primary performance metric. 
Only the configuration that resulted in the highest $F_1$-score was retained for the final model.
The \textit{Hyperopt} library also allowed us to define a search space and to specify the maximum number of iterations to perform.

\subsection{Generating Predictions}
After training, the optimized classifier was used to generate predictions on the test set. 
The predicted labels were obtained by applying the trained model to the feature set derived from the encoded test log. 
Similar to the training phase, the features $(\boldsymbol{X}_{test})$ were prepared by removing the trace identifier and ground truth label columns. 
The classifier's performance was subsequently evaluated using standard scikit-learn metrics, including \textit{Accuracy}, \textit{Precision}, \textit{Recall}, and the \textit{F1-score}.

\subsection{Extracting Recommendations}
Given the interpretability inherent in the decision tree structure, the model can provide proactive recommendations specifically for traces predicted to result in a negative outcome.
The process for generating these recommendations involved three main steps:
\begin{enumerate}
    \item \textbf{Extracting Positive Paths}: identify all distinct paths in the decision tree that lead to a leaf node associated with a positive outcome.
    \item \textbf{Filtering Compliant Paths}: for a given negatively-predicted trace prefix, filter the positive paths to find those that are compliant 
    (i.e., whose conditions are not violated by the activities already present in the trace).
    \item \textbf{Generating Recommendations}: from the compliant positive paths, select the one with the highest confidence score. Determine the set of 
    activities required to complete that path.
\end{enumerate}

\paragraph{Extracting Positive Paths from the Decision Tree:}
To extract all positive paths from the decision tree, we traversed the tree structure starting from the root node using a \textit{Depth-first search} (DFS) approach.
At each decision node in the path, we recorded the feature, the operator (e.g., $\le$, $>$), and the threshold that leads to the next node.
\\Upon reaching a leaf node, if the node's label corresponded to a positive outcome, the entire path, along with its associated confidence score, was stored as a positive path.
\begin{algorithm}[H]
    \label{alg:get_positive_paths}
    \caption{Get Positive Paths}
    \begin{algorithmic}[1]
    \Function{GetPositivePaths}{node, current\_path, positive\_paths}
        \If{node \textbf{is} leaf}
            \If{node.label == positive}
                \State Add (current\_path, node.confidence) to positive\_paths
            \EndIf
        \Else
            \State Append (node.feature, $\le$, node.value) to current\_path
            \State Call \textbf{GetPositivePaths}(node.left, current\_path, positive\_paths)
            \State Remove last element from current\_path
            \\
            \State Append (node.feature, $>$, node.value) to current\_path
            \State Call \textbf{GetPositivePaths}(node.right, current\_path, positive\_paths)
            
            \State Remove last element from current\_path
        \EndIf
    \EndFunction
    \end{algorithmic}
\end{algorithm}

\paragraph{Filtering compliant paths:}
After all positive paths were extracted, for each prefix trace predicted as negative, we filtered the set to retain only the \textit{compliant} paths. 
A path is defined as compliant if and only if none of the feature conditions along the path are violated by the set of activities currently present in the prefix trace. 
Among all compliant paths, the one with the highest confidence score was selected. In case of a tie in confidence, the path with the shortest length was chosen as the tie-breaker.

\paragraph{Generate Recommendations:}
Once the most suitable positive compliant path was identified, recommendations were generated by determining the activities that needed to be added to the current trace to follow the selected path.
For each test set prefix, the recommendation set is defined as:
\begin{itemize}
    \item The empty set if the prefix was predicted as positive or if no compliant positive path was found;
    \item The set of activities corresponding to the conditions in the selected positive path that were not satisfied by the current prefix trace otherwise.
\end{itemize}

\subsection{Evaluation of Recommendations}
To evaluate the quality of the recommendations generated, we compared them against the actual full traces in the test set, where the encoded test set served as the ground truth for comparison.
\\\\
For each complete trace in the test set, we first checked if the trace's prefix matched the specific conditions leading to a particular recommendation, as defined by the decision tree classifier.
Only if a prefix match occurred, we then proceeded to check if the recommended activity was actually performed in the subsequent step of the actual trace following that prefix.
This recommendation outcome was then compared with the ground truth outcome of the trace to determine the predictive accuracy of the recommendation.
Note that if a matched prefix had an empty recommendation, it was interpreted as no changes being necessary, and thus the recommendation was considered to be followed by default.
\\\\
With those checks we can create an approximated confusion matrix, where:
\begin{itemize}
    \item \textbf{True Positives}: The recommended activity was followed in the actual trace, and the ground truth outcome is positive.
    \item \textbf{False Positives}: The recommended activity was not followed in the actual trace, but the ground truth outcome is positive.
    \item \textbf{True Negatives}: The recommended activity was not followed in the actual trace, and the ground truth outcome is negative.
    \item \textbf{False Negatives}: The recommended activity was followed in the actual trace, but the ground truth outcome is negative.
\end{itemize}
Starting from this confusion matrix, we computed standard evaluation metrics such as accuracy, precision, recall, and F1-score to assess the effectiveness of the recommendations provided by our system.